{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nbhelper\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
    "from functools import reduce\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f74c41134a8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpYTTvhmZjnhhG9mlhP/H6RF+q0xZeS0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74c41cd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "\n",
    "# plot to show they're basically the same\n",
    "x = range(-10,10)\n",
    "plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')           # red  x\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate')  # blue +\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.legend(loc=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "\n",
    "    # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the gradient\n",
      "starting v [5, -6, 9]\n",
      "iterations 728\n",
      "minimum v ['0.0000020909', '-0.0000025090', '0.0000037636']\n",
      "minimum value 0.00000000002483146835\n"
     ]
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "#\n",
    "# Compute the minimum of the sum of squares among all three-dimensional vectors.\n",
    "#\n",
    "print(\"using the gradient\")\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "print(\"starting v\", v)\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "# do the gradient descent\n",
    "iterations_counter = 0\n",
    "while True:\n",
    "    iterations_counter += 1\n",
    "    #print(v, sum_of_squares(v))\n",
    "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance:     # stop if we're converging\n",
    "        break\n",
    "    v = next_v                              # continue if we're not\n",
    "\n",
    "print(\"iterations\", iterations_counter)\n",
    "print(\"minimum v\", [\"{:.10f}\".format(v_i) for v_i in v]) # print elements of v as floats with 10 digits after the dot\n",
    "print(\"minimum value {:.20f}\".format(sum_of_squares(v))) # print the value as a float with 20 digits after the dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Step Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# minimize / maximize batch\n",
    "#\n",
    "#\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using minimize_batch\n",
      "minimum v [0.0002596148429267415, -0.000519229685853483, -0.0012980742146337075]\n",
      "minimum value 0.0000020220\n"
     ]
    }
   ],
   "source": [
    "print(\"using minimize_batch\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value {:.10f}\".format(sum_of_squares(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# minimize / maximize stochastic\n",
    "#\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
