{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math, random, csv, json, re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delimited Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab delimited stock prices:\n",
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "6/19/2014 AAPL 91.86\n",
      "6/19/2014 MSFT 41.51\n",
      "6/19/2014 FB 64.34\n"
     ]
    }
   ],
   "source": [
    "def process(date, symbol, price):\n",
    "    print(date, symbol, price)\n",
    "\n",
    "print(\"tab delimited stock prices:\")\n",
    "\n",
    "# The book advises to work with CSV files in binary mode by including a \"b\" after the \"r\" or \"w\".\n",
    "# The advice holds for Python 2 but not for Python 3. Do not add \"b\" in Python 3 code.\n",
    "with open('tab_delimited_stock_prices.txt', 'r', encoding='utf8', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    # reader = csv.reader(codecs.iterdecode(f, 'utf-8'), delimiter='\\t')\n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colon delimited stock prices:\n",
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n"
     ]
    }
   ],
   "source": [
    "print(\"colon delimited stock prices:\")\n",
    "\n",
    "with open('colon_delimited_stock_prices.txt', 'r', encoding='utf8',newline='') as f:\n",
    "    reader = csv.DictReader(f, delimiter=':')\n",
    "    # reader = csv.DictReader(codecs.iterdecode(f, 'utf-8'), delimiter=':')\n",
    "    for row in reader:\n",
    "        date = row[\"date\"]\n",
    "        symbol = row[\"symbol\"]\n",
    "        closing_price = float(row[\"closing_price\"])\n",
    "        process(date, symbol, closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing out comma_delimited_stock_prices.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"writing out comma_delimited_stock_prices.txt\")\n",
    "\n",
    "today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "with open('comma_delimited_stock_prices.txt','w', encoding='utf8',newline='') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for stock, price in today_prices.items():\n",
    "        writer.writerow([stock, price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL,90.91\r",
      "\r\n",
      "MSFT,41.68\r",
      "\r\n",
      "FB,64.5\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%cat 'comma_delimited_stock_prices.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML and the Parsing Thereof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeautifulSoup\n",
      "<!DOCTYPE html>\n",
      "<html><head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\"/>\n",
      "    <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 50px;\n",
      "        background-color: #fff;\n",
      "        border-radius: 1em;\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        body {\n",
      "            background-color: #fff;\n",
      "        }\n",
      "        div {\n",
      "            width: auto;\n",
      "            margin: 0 auto;\n",
      "            border-radius: 0;\n",
      "            padding: 1em;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is established to be used for illustrative examples in documents. You may use this\n",
      "    domain in examples without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "\n",
      "\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "print(\"BeautifulSoup\")\n",
    "html = requests.get(\"http://www.example.com\").text\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Oâ€™Reilly Books About Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** This example does not work anymore, because the HTML code of the corresponding O'Reilly Web-page was changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def is_video(td):\n",
    "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
    "    the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "            pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "\n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "\n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def scrape(num_pages=31):\n",
    "    base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "           \"data.do?sortby=publicationDate&page=\"\n",
    "\n",
    "    books = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        print(\"souping page\", page_num)\n",
    "        url = base_url + str(page_num)\n",
    "        soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "        for td in soup('td', 'thumbtext'):\n",
    "            if not is_video(td):\n",
    "                books.append(book_info(td))\n",
    "\n",
    "        # now be a good citizen and respect the robots.txt!\n",
    "        sleep(30)\n",
    "\n",
    "    return books\n",
    "\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to\n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "def plot_years(plt, books):\n",
    "    # 2014 is the last complete year of data (when I ran this)\n",
    "    year_counts = Counter(get_year(book) for book in books\n",
    "                          if get_year(book) <= 2014)\n",
    "\n",
    "    years = sorted(year_counts)\n",
    "    book_counts = [year_counts[year] for year in x]\n",
    "    plt.bar([x - 0.5 for x in years], book_counts)\n",
    "    plt.xlabel(\"year\")\n",
    "    plt.ylabel(\"# of data books\")\n",
    "    plt.title(\"Data is Big!\")\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON (and XML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing json\n",
      "{'title': 'Data Science Book', 'author': 'Joel Grus', 'publicationYear': 2014, 'topics': ['data', 'science', 'data science']}\n"
     ]
    }
   ],
   "source": [
    "print(\"parsing json\")\n",
    "\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                  \"author\" : \"Joel Grus\",\n",
    "                  \"publicationYear\" : 2014,\n",
    "                  \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "# parse the JSON to create a Python object\n",
    "deserialized = json.loads(serialized)\n",
    "if \"data science\" in deserialized[\"topics\"]:\n",
    "    print(deserialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Unauthenticated API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub API\n",
      "dates [datetime.datetime(2017, 12, 2, 20, 13, 49, tzinfo=tzutc()), datetime.datetime(2017, 12, 19, 0, 12, 40, tzinfo=tzutc()), datetime.datetime(2013, 7, 5, 2, 2, 28, tzinfo=tzutc()), datetime.datetime(2017, 5, 10, 17, 22, 45, tzinfo=tzutc()), datetime.datetime(2013, 11, 15, 5, 33, 22, tzinfo=tzutc()), datetime.datetime(2012, 9, 18, 4, 20, 23, tzinfo=tzutc()), datetime.datetime(2016, 7, 19, 17, 34, 31, tzinfo=tzutc()), datetime.datetime(2015, 11, 11, 14, 15, 36, tzinfo=tzutc()), datetime.datetime(2016, 5, 31, 14, 33, 6, tzinfo=tzutc()), datetime.datetime(2015, 6, 30, 0, 33, 3, tzinfo=tzutc()), datetime.datetime(2013, 8, 21, 13, 26, 5, tzinfo=tzutc()), datetime.datetime(2013, 8, 18, 5, 3, 41, tzinfo=tzutc()), datetime.datetime(2015, 7, 30, 1, 54, 55, tzinfo=tzutc()), datetime.datetime(2014, 11, 9, 2, 31, 24, tzinfo=tzutc()), datetime.datetime(2013, 11, 10, 6, 52, 56, tzinfo=tzutc()), datetime.datetime(2015, 4, 8, 1, 1, 47, tzinfo=tzutc()), datetime.datetime(2016, 1, 8, 3, 33, 58, tzinfo=tzutc()), datetime.datetime(2016, 1, 21, 6, 46, 49, tzinfo=tzutc()), datetime.datetime(2013, 7, 1, 3, 36, 23, tzinfo=tzutc()), datetime.datetime(2013, 2, 22, 0, 12, 38, tzinfo=tzutc()), datetime.datetime(2016, 5, 21, 23, 57, 23, tzinfo=tzutc()), datetime.datetime(2015, 7, 2, 21, 47, 55, tzinfo=tzutc()), datetime.datetime(2016, 10, 23, 21, 28, 37, tzinfo=tzutc()), datetime.datetime(2012, 2, 15, 4, 55, 49, tzinfo=tzutc()), datetime.datetime(2017, 11, 21, 23, 59, 6, tzinfo=tzutc()), datetime.datetime(2017, 5, 23, 14, 0, 58, tzinfo=tzutc()), datetime.datetime(2016, 7, 11, 15, 48, 39, tzinfo=tzutc()), datetime.datetime(2016, 1, 20, 15, 28, 1, tzinfo=tzutc()), datetime.datetime(2013, 7, 4, 17, 28, 29, tzinfo=tzutc()), datetime.datetime(2013, 12, 8, 17, 53, 7, tzinfo=tzutc())]\n",
      "month_counts Counter({7: 7, 11: 5, 5: 4, 12: 3, 1: 3, 8: 2, 2: 2, 9: 1, 6: 1, 4: 1, 10: 1})\n",
      "weekday_count Counter({1: 7, 2: 6, 6: 5, 4: 4, 3: 4, 5: 2, 0: 2})\n",
      "last five languages ['Python', 'Python', 'Python', 'Jupyter Notebook', 'JavaScript']\n"
     ]
    }
   ],
   "source": [
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "print(\"GitHub API\")\n",
    "print(\"dates\", dates)\n",
    "print(\"month_counts\", month_counts)\n",
    "print(\"weekday_count\", weekday_counts)\n",
    "\n",
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r: r[\"created_at\"],\n",
    "                             reverse=True)[:5]\n",
    "\n",
    "print(\"last five languages\", [repo[\"language\"]\n",
    "                              for repo in last_5_repositories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using the Twitter APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "\n",
    "# fill these in if you want to use the code\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "def call_twitter_search_api():\n",
    "\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "    # search for tweets containing the phrase \"data science\"\n",
    "    for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "        user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "        text = status[\"text\"].encode('utf-8')\n",
    "        print(user, \":\", text)\n",
    "        print()\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "\n",
    "# appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "tweets = []\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"our own subclass of TwythonStreamer that specifies\n",
    "    how to interact with the stream\"\"\"\n",
    "\n",
    "    def on_success(self, data):\n",
    "        \"\"\"what do we do when twitter sends us data?\n",
    "        here data will be a Python object representing a tweet\"\"\"\n",
    "\n",
    "        # only want to collect English-language tweets\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "\n",
    "        # stop when we've collected enough\n",
    "        if len(tweets) >= 1000:\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "def call_twitter_streaming_api():\n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
